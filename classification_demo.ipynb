{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PyTorch Classification Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataset: GroZi-120 dataset (http://grozi.calit2.net/), containing 120 product labels, 676 training data and 29 videos as evaluation data. The videos can be extracted as individual product images (this is provided in the dataset's website).\n",
    "* Clone https://github.com/jonathan016/rpdr-config-results for dataset and result folder locations and put it at the same level as this project's location. **This is required for the notebook to run.**\n",
    "    - Run `git clone https://github.com/jonathan016/rpdr-config-results.git` from your command line runner/terminal\n",
    "* This notebook requires **at least** `Python 3.6.2` and the following library versions:\n",
    "    - `notebook >= 5.2.2`\n",
    "    - `torch >= 1.3.1`\n",
    "    - `torchvision >= 0.4.2`\n",
    "    - `pillow >= 6.2.1`\n",
    "* To open this notebook in interactive mode, after cloning this repository (`git clone https://github.com/jonathan016/rpdr.git`), navigate to the cloned folder, then run `pip install notebook` and `jupyter notebook` from your command line runner/terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python-provided libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from random import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External libraries\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn.functional as torch_fn\n",
    "\n",
    "from PIL import ImageFilter\n",
    "from torch.nn import CrossEntropyLoss, Linear, Module, Conv2d, MaxPool2d\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor, ColorJitter, Normalize, Lambda, \\\n",
    "    RandomResizedCrop, RandomErasing, RandomRotation, RandomPerspective, ToPILImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-run Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_location = './'\n",
    "train_root = '../rpdr-config-results/data/cropped'\n",
    "eval_root = '../rpdr-config-results/data/in_situ_jpgs'\n",
    "eval_indices = '../rpdr/val_test/recog_val_test.json'\n",
    "eval_files = './val_test/recog_val_test_classes_files.json'\n",
    "logfile = '../rpdr-config-results/results/ssd/base/3.log'\n",
    "save = '../rpdr-config-results/results/ssd/base/3_300.pth.tar'\n",
    "best = '../rpdr-config-results/results/ssd/base/3_300_best_model.pth.tar'\n",
    "force_cuda = False  # TODO Set true if you have CUDA driver (only on NVIDIA GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(module_location)\n",
    "from utils.datasets import UsageBasedDataset, RecognitionDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_blur(image):\n",
    "    return image.filter(ImageFilter.BoxBlur(randint(0, 7)))\n",
    "\n",
    "\n",
    "def maybe_random_crop(image):\n",
    "    if randint(0, 100) <= 35:\n",
    "        return RandomResizedCrop(size=image.size, scale=(0.5, 1.0), ratio=(1., 1.))(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def maybe_random_erase(image):\n",
    "    if randint(0, 100) <= 7:\n",
    "        return ToPILImage()(RandomErasing(p=1.)(ToTensor()(image)))\n",
    "    return image\n",
    "\n",
    "\n",
    "def maybe_rotate(image):\n",
    "    if randint(0, 100) <= 4:\n",
    "        r = randint(0, 100)\n",
    "        if r <= 20:\n",
    "            return RandomRotation(degrees=90)(image)\n",
    "        elif r <= 50:\n",
    "            return RandomRotation(degrees=45)(image)\n",
    "        else:\n",
    "            return RandomRotation(degrees=30)(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def maybe_random_perspective(image):\n",
    "    if randint(0, 100) <= 2:\n",
    "        return RandomPerspective(distortion_scale=randint(4, 10) / 10, p=1.)(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def maybe_darken_a_lot(image):\n",
    "    if randint(0, 100) <= 30:\n",
    "        brightness = uniform(.5, .8)\n",
    "        saturation = uniform(1., 1.5)\n",
    "        return ColorJitter(brightness=(brightness, brightness), saturation=(saturation, saturation))(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out(value):\n",
    "    global logfile\n",
    "\n",
    "    if logfile:\n",
    "        print(value, file=open(logfile, 'a'))\n",
    "    else:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function\n",
    "\n",
    "Useful for both validation and testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, criterion, loader):\n",
    "    global force_cuda\n",
    "\n",
    "    # Set evaluation flag on model. Makes all parameters non-trainable/non-adjustable during evaluation\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize evaluation variables\n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate through evaluation data\n",
    "    for i, data in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Transfer to CUDA device for faster execution, if CUDA device is available\n",
    "            if force_cuda and cuda.is_available():\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Do forward pass, obtain predictions, then calculate loss\n",
    "            outputs = model(inputs)\n",
    "            _, prediction = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update training variables for logging purposes\n",
    "            loss += loss.data.item()\n",
    "            acc += torch.sum(prediction == labels.data).item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Clear memory of all processed data in this iteration\n",
    "            del inputs, labels, outputs, prediction\n",
    "            cuda.empty_cache()\n",
    "\n",
    "    # Calculate average loss and average accuracy for logging purposes\n",
    "    avg_loss = loss / total\n",
    "    avg_acc = acc / total\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, epoch, train_data, val_data, save_file, best_file):\n",
    "    global force_cuda\n",
    "\n",
    "    # Check for existing best model's weight file and load it if it exists\n",
    "    if os.path.exists(best_file):\n",
    "        out('Loading best model')\n",
    "        loader = torch.load(best_file)\n",
    "        model.load_state_dict(loader['state_dict'])\n",
    "        best_val_acc = loader['best_val']\n",
    "    else:\n",
    "        best_val_acc = None\n",
    "\n",
    "    # Set training flag on model. Makes all parameters adjustable during training\n",
    "    model.train()\n",
    "\n",
    "    # Initialize training variables\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_img = 0\n",
    "    iteration_losses = []\n",
    "\n",
    "    # Iterate through training data\n",
    "    for i, data in enumerate(train_data):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Transfer to CUDA device for faster execution, if CUDA device is available\n",
    "        if force_cuda and cuda.is_available():\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Empty gradient/loss in the model parameters (encapsulated in the optimizer; search `Construct Optimizer` in this notebook).\n",
    "        # Required by default in PyTorch to 'clean' the model's loss buffer before any new loss calculation and backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Do forward pass, obtain predictions, then calculate loss\n",
    "        outputs = model(inputs)\n",
    "        _, predictions = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Compute loss for all model parameters\n",
    "        loss.backward()\n",
    "        # Do backpropagation (updates model parameters)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update training variables for logging purposes\n",
    "        iteration_losses.append(loss.data.item())\n",
    "        total_loss += loss.data.item()\n",
    "        total_acc += torch.sum(predictions == labels.data).item()\n",
    "        total_img += labels.size(0)\n",
    "\n",
    "        # Clear memory of all processed data in this iteration\n",
    "        del inputs, labels, outputs, predictions\n",
    "        cuda.empty_cache()\n",
    "\n",
    "    # Log training statistics for specified epoch\n",
    "    out(f'Training #{epoch}: {total_acc / total_img} accuracy and {total_loss / total_img} loss')\n",
    "    \n",
    "    # Do validation\n",
    "    val_loss, val_acc = eval(model, criterion, val_data)\n",
    "    out(f'Validation #{epoch}: {val_acc} accuracy and {val_loss} loss')\n",
    "\n",
    "    # Select and save best model's weight by validation accuracy\n",
    "    if best_val_acc is None or best_val_acc < val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_val': best_val_acc\n",
    "        }, best_file)\n",
    "\n",
    "    # Save per epoch\n",
    "    saved_iteration_losses = iteration_losses\n",
    "    if os.path.exists(save_file):\n",
    "        saved_iteration_losses = torch.load(save_file)['iteration_losses']\n",
    "        saved_iteration_losses.extend(iteration_losses)\n",
    "\n",
    "    # Save checkpoint per training epoch\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'criterion': criterion.state_dict(),\n",
    "        'iteration_losses': saved_iteration_losses,\n",
    "        'last_epoch': epoch\n",
    "    }, save_file)\n",
    "\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_base = vgg16(pretrained=True)\n",
    "\n",
    "# Replace final classifier (fully connected) layer to output desired number of class scores (in this case, 120)\n",
    "vgg16_base.classifier[6] = Linear(4096, 120)\n",
    "\n",
    "# You're familiar with this one.\n",
    "if force_cuda and cuda.is_available():\n",
    "    vgg16_base = vgg16_base.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(vgg16_base.parameters(), lr=.001, momentum=.9, weight_decay=.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Training and Validation Data Feeder\n",
    "\n",
    "Bear with me on this one.\n",
    "\n",
    "We use the term `Data Feeder` as PyTorch uses the term `Dataset` as a class which cannot be used for mini-batch training. To use mini-batch training, we need to construct a `DataLoader` instance to be iterated on later in the `train` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation to be used to augment the data on request (on-the-fly/on-line augmentation)\n",
    "train_transform = Compose([\n",
    "    Lambda(maybe_blur),\n",
    "    Lambda(maybe_darken_a_lot),\n",
    "    Lambda(maybe_rotate),\n",
    "    Lambda(maybe_random_perspective),\n",
    "    Lambda(maybe_random_crop),\n",
    "    Lambda(maybe_random_erase),\n",
    "    ColorJitter(brightness=(.1, .8), contrast=.05, saturation=.05, hue=.005),\n",
    "    Resize(size=(300, 300)),\n",
    "    Grayscale(num_output_channels=3),\n",
    "    ToTensor(),\n",
    "    Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "])\n",
    "\n",
    "# Create the dataset. `UsageBasedDataset` is a custom class made for automatic balancing of data\n",
    "train_dataset = UsageBasedDataset(train_root, usage=150, transform=train_transform)\n",
    "\n",
    "# Create the `DataLoader` instance with batch size of 8. Google the rest of the method parameters for more information\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True, drop_last=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation to be used to transform the evaluation data on request (on-the-fly/on-line augmentation). This is not an\n",
    "# augmentation transformation.\n",
    "eval_transform = Compose([Resize(size=(300, 300)), Grayscale(num_output_channels=3), ToTensor(), Normalize((.5, .5, .5), (.5, .5, .5))])\n",
    "\n",
    "# Create the dataset. `RecognitionDataset` is a custom class made for automatic selection of validation or testing data on\n",
    "# GroZi-120 dataset\n",
    "val_dataset = RecognitionDataset(eval_root, eval_indices, eval_files, RecognitionDataset.VAL, transform=eval_transform)\n",
    "\n",
    "# Create the `DataLoader` instance with batch size of 1\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Finally) Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint, if exists before training. This alters the starting epoch as training have elapsed until the checkpoint's\n",
    "# specified epoch\n",
    "if os.path.exists(save):\n",
    "    saved_checkpoint = torch.load(save)\n",
    "    start_epoch = saved_checkpoint['last_epoch'] + 1\n",
    "    sgd.load_state_dict(saved_checkpoint['optimizer'])\n",
    "    crit.load_state_dict(saved_checkpoint['criterion'])\n",
    "else:\n",
    "    start_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training for 75 epochs. The number `76` is due to starting epoch starts at 1, so 1 + 75 = ? (You're smart)\n",
    "start_time = time.time()\n",
    "for epoch in range(start_epoch, 76):\n",
    "    vgg16_base, sgd, crit = train(vgg16_base, sgd, crit, epoch, train_loader, val_loader, save, best)\n",
    "end_time = time.time()\n",
    "out(f'VGG base recognition training elapsed for {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Testing Data Feeder (Sounds familiar?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = RecognitionDataset(eval_root, eval_indices, eval_files, RecognitionDataset.TEST, transform=eval_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model's weights to be used for testing\n",
    "vgg16_base.load_state_dict(torch.load(best)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_loss, test_acc = eval(vgg16_base, crit, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log results\n",
    "out('\\n=========')\n",
    "out(f'Test Average Loss: {test_loss}')\n",
    "out(f'Test Average Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goodluck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
